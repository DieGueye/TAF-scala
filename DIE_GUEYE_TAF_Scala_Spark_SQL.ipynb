{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data avec Spark : Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`Nom & Prenom : GUEYE DIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problematique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce projet consiste à utiliser Apache Spark pour faire l'analyse et le traitement des données de **[San Francisco Fire Department Calls ](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)** afin de fournir quelques KPI (*Key Performance Indicator*). Le **SF Fire Datasets** comprend les réponses aux appels de toutes les unités d'incendie. Chaque enregistrement comprend le numéro d'appel, le numéro d'incident, l'adresse, l'identifiant de l'unité, le type d'appel et la disposition. Tous les intervalles de temps pertinents sont également inclus. Étant donné que ce Dataset est basé sur les réponses et que la plupart des appels impliquent plusieurs unités, ainsi il existe plusieurs enregistrements pour chaque numéro d'appel. Les adresses sont associées à un numéro de bloc, à une intersection ou à une boîte d'appel, et non à une adresse spécifique.\n",
    "\n",
    "**Plus de details sur la description des données [ici](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)**\n",
    "\n",
    "**Download csv file [here](https://data.sfgov.org/api/views/nuek-vuh3/rows.csv?accessType=DOWNLOAD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail à faire.\n",
    "L'objectif de ce travail est de comprendre le Dataset SF Fire afin de bien répondre aux questions en utilisant les codes Spark/Scala adéquats.\n",
    "\n",
    "- Code lisible et bien indenté, \n",
    "- N'oublier pas de mettre en commentaire la justification de votre réponse sur les cellule Markdown. \n",
    "\n",
    "\n",
    "#### Note:\n",
    "- Vous pouvez en groupe (au plus deux étudiants) . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Importez les modules Spark necessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // Not required since almond 0.7.0 (will be automatically added when importing spark)\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` // Or use any other 2.x version here\n",
    "import $ivy.`sh.almond::almond-spark:0.10.9` // Not required since almond 0.7.0 (will be automatically added when importing spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mrootLogger\u001b[39m: \u001b[32mLogger\u001b[39m = org.apache.log4j.spi.RootLogger@409e56a0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val rootLogger = Logger.getRootLogger()\n",
    "rootLogger.setLevel(Level.ERROR)\n",
    "\n",
    "Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN)\n",
    "Logger.getLogger(\"org.spark-project\").setLevel(Level.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Creez la Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@207ccb20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = {\n",
    "  SparkSession.builder()\n",
    "    .master(\"local\")\n",
    "    .appName(\"BD-FS FIRE\")\n",
    "    .config(\"spark.some.option.config\", \"config-value\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Chargez les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/departement_call/sf-fire-calls.csv\"\u001b[39m\n",
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"data/departement_call/sf-fire-calls.csv\"\n",
    "val df = spark.read\n",
    "        .option(\"header\",\"true\")\n",
    "        .csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez le `fireSchema` definit dans la cellule suivante pour le chargement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mfireSchema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallNumber\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitID\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"IncidentNumber\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallType\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallDate\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"WatchDate\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallFinalDisposition\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"AvailableDtTm\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"City\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Zipcode\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Battalion\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StationArea\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Box\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"OriginalPriority\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Priority\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"FinalPriority\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"ALSUnit\"\u001b[39m, BooleanType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallTypeGroup\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"NumAlarms\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitType\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitSequenceInCallDispatch\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"FirePreventionDistrict\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"SupervisorDistrict\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Neighborhood\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Location\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"RowID\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Delay\"\u001b[39m, FloatType, true, {})\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    "  StructField(\"UnitID\", StringType, true),\n",
    "  StructField(\"IncidentNumber\", IntegerType, true),\n",
    "  StructField(\"CallType\", StringType, true),                  \n",
    "  StructField(\"CallDate\", StringType, true),      \n",
    "  StructField(\"WatchDate\", StringType, true),\n",
    "  StructField(\"CallFinalDisposition\", StringType, true),\n",
    "  StructField(\"AvailableDtTm\", StringType, true),\n",
    "  StructField(\"Address\", StringType, true),       \n",
    "  StructField(\"City\", StringType, true),       \n",
    "  StructField(\"Zipcode\", IntegerType, true),       \n",
    "  StructField(\"Battalion\", StringType, true),                 \n",
    "  StructField(\"StationArea\", StringType, true),       \n",
    "  StructField(\"Box\", StringType, true),       \n",
    "  StructField(\"OriginalPriority\", StringType, true),       \n",
    "  StructField(\"Priority\", StringType, true),       \n",
    "  StructField(\"FinalPriority\", IntegerType, true),       \n",
    "  StructField(\"ALSUnit\", BooleanType, true),       \n",
    "  StructField(\"CallTypeGroup\", StringType, true),\n",
    "  StructField(\"NumAlarms\", IntegerType, true),\n",
    "  StructField(\"UnitType\", StringType, true),\n",
    "  StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "  StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "  StructField(\"SupervisorDistrict\", StringType, true),\n",
    "  StructField(\"Neighborhood\", StringType, true),\n",
    "  StructField(\"Location\", StringType, true),\n",
    "  StructField(\"RowID\", StringType, true),\n",
    "  StructField(\"Delay\", FloatType, true)))\n",
    "\n",
    "// your code here (hint spark session name is sparkSession Q2)\n",
    "// val data = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Mettez en cache les donnees chargees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/17 22:28:10 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7_0\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]\n",
       "\u001b[36mres7_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//On utilise la mise en cache quand on effectue plusieurs actions sur le même DataFrame. \n",
    "df.cache\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Supprimez tous les appels de type `Medical Incident`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: appliquez la methode `.filter()` a la colonne `CallType` avec l'operateur `=!=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/17 22:28:14 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        CallType|\n",
      "+----------------+\n",
      "|  Structure Fire|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|    Vehicle Fire|\n",
      "|          Alarms|\n",
      "|  Structure Fire|\n",
      "|          Alarms|\n",
      "|          Alarms|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|  Structure Fire|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|  Structure Fire|\n",
      "|  Structure Fire|\n",
      "|  Structure Fire|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Avant la suppression des lignes contenant tous les appels de type \"Medical Incident\"\n",
    "df.select(\"CallType\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.col\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mdf_filtered\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//La méthode col() permet de sélectionner la colonne à filtrer et l'opérateur =!= est utilisé pour filtrer les valeurs dont CallType est egale à \"Medical Incident\"\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val df_filtered = df.filter(col(\"CallType\") =!= \"Medical Incident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Combien de types d'appels distincts ont été passés ?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.countDistinct\n",
       "\u001b[39m\n",
       "\u001b[36mres10_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Nombre de type d'appel: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Pour obtenir les valeurs distinctes dans la colonne CaalType on utilise la méthode distinct() \n",
    "//et la méthode count() est utilisée pour compter le nombre de valeurs distinctes. \n",
    "//Ainsi on obtient 30 valeurs distinctes\n",
    "import org.apache.spark.sql.functions.countDistinct\n",
    "df_filtered.agg(countDistinct(\"CallType\") as \"Nombre de type d'appel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Quels types d'appels  ont été passés au service d'incendie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|CallType                                    |\n",
      "+--------------------------------------------+\n",
      "|Elevator / Escalator Rescue                 |\n",
      "|Marine Fire                                 |\n",
      "|Aircraft Emergency                          |\n",
      "|Confined Space / Structure Collapse         |\n",
      "|Administrative                              |\n",
      "|Alarms                                      |\n",
      "|Odor (Strange / Unknown)                    |\n",
      "|Citizen Assist / Service Call               |\n",
      "|HazMat                                      |\n",
      "|Watercraft in Distress                      |\n",
      "|Explosion                                   |\n",
      "|Oil Spill                                   |\n",
      "|Vehicle Fire                                |\n",
      "|Suspicious Package                          |\n",
      "|Extrication / Entrapped (Machinery, Vehicle)|\n",
      "|Other                                       |\n",
      "|Outside Fire                                |\n",
      "|Traffic Collision                           |\n",
      "|Assist Police                               |\n",
      "|Gas Leak (Natural and LP Gases)             |\n",
      "|Water Rescue                                |\n",
      "|Electrical Hazard                           |\n",
      "|High Angle Rescue                           |\n",
      "|Structure Fire                              |\n",
      "|Industrial Accidents                        |\n",
      "|Mutual Aid / Assist Outside Agency          |\n",
      "|Fuel Spill                                  |\n",
      "|Smoke Investigation (Outside)               |\n",
      "|Train / Rail Incident                       |\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered.select(\"CallType\").distinct.show(30, truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. Trouvez toutes les réponses ou les délais sont supérieurs à 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint:\n",
    "1. Renommez la colonne `Delay` -> `ReponseDelayedinMins`\n",
    "2. Retournez un nouveau DataFrame\n",
    "3. Affichez tous les appels où le temps de réponse au site d'incendie a eu un retard de plus de 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mnewdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Pour renommez une colonne on utilise la fonction withColumnRenamed()\n",
    "val newdf = df_filtered.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: string (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: string (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: string (nullable = true)\n",
      " |-- ALSUnit: string (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: string (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: string (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- ResponseDelayedinMins: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Verification\n",
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres38\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Affichons tous les appels où le temps de réponse au site d'incendie a eu un retard de plus de 5 minutes\n",
    "newdf.filter(col(\"ResponseDelayedinMins\")>5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. Convertissez les colonnes dates en timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint:\n",
    "* `CallDate` -> `IncidentDate`\n",
    "* `WatchDate` -> `OnWatchDate`\n",
    "* `AvailableDtTm` -> `AvailableDtTS`\n",
    "exemple code pour le cas de `CallDate`:\n",
    "`dataframe.withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{col, to_timestamp}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mnewdf2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, to_timestamp}\n",
    "\n",
    "val newdf2 = newdf\n",
    "  .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    "  .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    "  .withColumn(\"AvailableDt\", to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy\"))\n",
    "  .drop(\"CallDate\", \"WatchDate\", \"AvailableDtTm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: string (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: string (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: string (nullable = true)\n",
      " |-- ALSUnit: string (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: string (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: string (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- ResponseDelayedinMins: string (nullable = true)\n",
      " |-- IncidentDate: timestamp (nullable = true)\n",
      " |-- OnWatchDate: timestamp (nullable = true)\n",
      " |-- AvailableDt: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Verification\n",
    "newdf2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. Quels sont les types d'appels les plus courants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            CallType|count|\n",
      "+--------------------+-----+\n",
      "|      Structure Fire|23319|\n",
      "|              Alarms|19406|\n",
      "|   Traffic Collision| 7013|\n",
      "|Citizen Assist / ...| 2524|\n",
      "|               Other| 2166|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{col, desc}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//La méthode groupBy() permet de regrouper les lignes de la base de données en fonction des valeurs de la colonne \n",
    "//\"CallType\". La méthode count() est ensuite utilisée pour compter le nombre de lignes pour chaque groupe et \n",
    "//la méthode orderBy() est utilisée pour trier les résultats par ordre décroissant de nombre d'appels.\n",
    "import org.apache.spark.sql.functions.{col, desc}\n",
    "val topCalls = newdf.groupBy(\"CallType\").count().orderBy(desc(\"count\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11. Quels sont les boites postales rencontrées dans les appels les plus courants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Zipcode|\n",
      "+-------+\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf\n",
    "    .filter(col(\"CallType\") === \"Medical Incident\")\n",
    "    .select(\"Zipcode\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12. Quels sont les quartiers de San Francisco dont les codes postaux sont `94102` et `94103`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|        Neighborhood|Zipcode|\n",
      "+--------------------+-------+\n",
      "|         Mission Bay|  94103|\n",
      "|Financial Distric...|  94103|\n",
      "| Castro/Upper Market|  94103|\n",
      "|    Western Addition|  94102|\n",
      "|            Nob Hill|  94102|\n",
      "|     South of Market|  94103|\n",
      "|        Potrero Hill|  94103|\n",
      "|        Hayes Valley|  94103|\n",
      "|     South of Market|  94102|\n",
      "|          Tenderloin|  94102|\n",
      "|          Tenderloin|  94103|\n",
      "|             Mission|  94103|\n",
      "|Financial Distric...|  94102|\n",
      "|        Hayes Valley|  94102|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf\n",
    "    .select(\"Neighborhood\",\"Zipcode\")\n",
    "    .where (col(\"Zipcode\") === 94102 || \n",
    "           col(\"Zipcode\") === 94103)\n",
    "    .distinct()\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13. Determinez le nombre total d'appels, ainsi que la moyenne, le minimum et le maximum du temps de réponse des appels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+\n",
      "|summary|ResponseDelayedinMins|\n",
      "+-------+---------------------+\n",
      "|  count|                61502|\n",
      "|   mean|   3.9247360518539818|\n",
      "| stddev|   14.446973866076661|\n",
      "|    min|          0.016666668|\n",
      "|    max|             99.63333|\n",
      "+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.select(\"ResponseDelayedinMins\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q14. Combien d'années distinctes trouve t-on dans ce Dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Appliquer la fonction `year()` a la colonne `IncidentDate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mres49_2\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1L\u001b[39m"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Pour déterminer le nombre d'années distinctes dans ce Dataset, nous pouvons sélectionner la colonne \"IncidentDate\", \n",
    "//extraire l'année de chaque date avec la fonction year() et utiliser la fonction distinct() pour obtenir les années \n",
    "//uniques. Ensuite, nous pouvons utiliser la méthode count() pour connaître le nombre d'années distinctes.\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import spark.implicits._\n",
    "newdf\n",
    "    .select(year(col(\"CallDate\")))\n",
    "    .distinct()\n",
    "    .count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q15. Quelle semaine de l'année 2018 a eu le plus d'appels d'incendie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Nombre de semaine|count|\n",
      "+-----------------+-----+\n",
      "|               25|   31|\n",
      "+-----------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf2\n",
    "    .filter(year(col(\"IncidentDate\")) === \"2018\" && col(\"callType\") === \"Structure Fire\")\n",
    "    .groupBy(weekofyear(col(\"IncidentDate\")).as(\"Nombre de semaine\"))\n",
    "    .count()\n",
    "    .orderBy(col(\"count\").desc)\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat ci dessus montre que la 1er semaine de l’année 2018 a eu le plus d'appels d'incendie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q16. Quels sont les quartiers de San Francisco qui ont connu le pire temps de réponse en 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|       Neighborhood|   AvgResponseTime|\n",
      "+-------------------+------------------+\n",
      "|           Seacliff|2.6428571285714284|\n",
      "|       Hayes Valley|2.6988888820000003|\n",
      "|   Western Addition|2.8509615380769233|\n",
      "|          Excelsior|2.9350340020408163|\n",
      "|          Japantown|2.9442029130434784|\n",
      "|       Lincoln Park| 2.944444466666667|\n",
      "|  Lone Mountain/USF|3.0205555449999997|\n",
      "|   Presidio Heights|      3.1759999776|\n",
      "|           Nob Hill|3.1841121410280384|\n",
      "|             Marina|3.3417670865060245|\n",
      "|            Mission|3.4078787935909083|\n",
      "|        North Beach|3.5173611426388884|\n",
      "|    Sunset/Parkside|3.5372641693396236|\n",
      "|    South of Market|3.5448660579017854|\n",
      "|     Outer Richmond| 3.551704540227272|\n",
      "|Castro/Upper Market|3.5757777834666666|\n",
      "|       Inner Sunset|        3.61604935|\n",
      "|     Inner Richmond| 3.619858159361702|\n",
      "|            Portola|3.7143677944827598|\n",
      "|          Glen Park|3.8864583062499993|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcalls2018\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallNumber: string, UnitID: string ... 26 more fields]\n",
       "\u001b[36mavgResponseByNeighborhood\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Neighborhood: string, AvgResponseTime: double]\n",
       "\u001b[36mworstResponseNeighborhoods\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Neighborhood: string, AvgResponseTime: double]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Filtrer les appels d'incendie de l'année 2018\n",
    "val calls2018 = newdf2.filter(year($\"IncidentDate\") === 2018)\n",
    "\n",
    "// Grouper par quartier et calculer la moyenne du temps de réponse\n",
    "val avgResponseByNeighborhood = calls2018.groupBy($\"Neighborhood\").agg(avg($\"ResponseDelayedinMins\").alias(\"AvgResponseTime\"))\n",
    "\n",
    "// Trier les résultats par ordre décroissant du temps de réponse\n",
    "val worstResponseNeighborhoods = avgResponseByNeighborhood.sort($\"AvgResponseTime\".asc)\n",
    "\n",
    "// Afficher les quartiers avec le pire temps de réponse\n",
    "worstResponseNeighborhoods.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q17. Stocker les données sous format de fichiers Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: path file:/Users/macbookpro/Desktop/bureau/IAM/Scala/bigdata/tmp/parket/files already exists.;\u001b[39m\n  org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(\u001b[32mInsertIntoHadoopFsRelationCommand.scala\u001b[39m:\u001b[32m114\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m102\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m122\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m83\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.toRdd(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m81\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m75\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m290\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m271\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m229\u001b[39m)\n  ammonite.$sess.cmd89$Helper.<init>(\u001b[32mcmd89.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd89$.<init>(\u001b[32mcmd89.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd89$.<clinit>(\u001b[32mcmd89.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "newdf2.write.format(\"parquet\").save(\"tmp/parket/files/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q18. Rechargez  les données stockées en format Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mnewdataDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newdataDF = spark.read.format(\"parquet\").load(\"tmp/parket/files/\")\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: string (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: string (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: string (nullable = true)\n",
      " |-- ALSUnit: string (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: string (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: string (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- ResponseDelayedinMins: string (nullable = true)\n",
      " |-- IncidentDate: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdataDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
